---
title: "ComputerIntensiveMethodsPractice"
author: "Olusoji Oluwafemi Daniel and Owokomo Olajumoke Evangelina"
date: "21 September 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
library(tidyr)
library(purrr)
library(magrittr)
library(dplyr)
library(tibble)
```

## Summary

All codes and write ups here are done to practice the concepts taught in Computer Intensive Methods and also to refine some of the codes used in the course using the `purrr` package.

## Sampling Distributions

```{r}

```

## Constructing The Empirical Distribution

The empirical distribution function is a function that assigns the probability $\frac{1}{n}$ to each sample value. It can be used to approximate the distribution function of the random variable in question.

### Using a Generated Sequnece

```{r}
#generate ssome random sequence
x <- sort(seq(-5,5,length=1000))
#probability of each value in x
dx <- dnorm(x)
#Density plot of x
plot(x,dx,type='l')
#Probability Distribution
px <- pnorm(x)
#plot of the Probability Distribution
plot(x,px,type='l')
```

### Using Simulated Values From a Normal Distribution

```{r}
#simulate 1000 values from the normal distribution and sort them
nv <- sort(rnorm(1000,0,1))
# Obtaining the Distribution Function
nvp <- pnorm(nv)
#plotting the distribution function
plot(nv,nvp,type='l')
##getting the empirical distribution
emnv <- (1:length(nv))/length(nv)
lines(nv,emnv,col=3,lwd=2)
```

It is seen that the empirical distribution properly approximates the distribution function even though it is a discrete distribution that assigns an equal probability $\frac{1}{n}$ to all samples.

## Bootstrapping


```{r}
# A sample of 10 shoe sizes
ss <- c(11.201, 10.035, 11.118, 9.055, 9.434, 9.663, 10.403, 11.662, 9.285,8.84)
#summary statistics of shoe size
mean(ss)
var(ss)
sd(ss)
#variacne of the sample mean
var(ss)/length(ss)
#standard error 
sd(ss)/sqrt(length(ss))
```

### Non Parametric Bootstrap of The Standard Error of the Mean

In non parametric bootstrapp, sampling is done from the empirical distribution contrary to the parametric bootstrapp where sampling is done from an assumed distribution and the parameters of such distribution is replaced by the MLE estimates.

```{r}
n <- length(ss)
B <- 1000
mss <- numeric(B)
mss2 <- numeric(B)
#bootstrapping algorithm
set.seed(1000)
for(i in 1:B){
  sams <- sample(ss,n,replace = T)
  mss[[i]] <- mean(sams)
}
#
mean(mss)
#
var(mss)
#
sd(mss)
#or
test <- unlist(map(1:B,function(i){sams <- sample(ss,n,replace = T)
mss2[[i]] <- mean(sams)}))
```

### Distribution of the Nonparametric Bootstrapp Estimates

```{r}
hist(mss)
```

### Parametric Bootstrapp

It can be assumed that the shoe size is from a normally distributed population where the mean and variance can be replaced with their plug in estimates (`r mean(ss)` and `r var(ss)`). Rather than take random samples from the empirical distribution, samples are taken from the distribution function whose parameters are the plug in estimates, i.e. N(`r mean(ss)`, `r var(ss)`).

```{r}
n <- length(ss)
B <- 1000
mss <- numeric(B)
mss2 <- numeric(B)
#bootstrapping algorithm
set.seed(1000)
for(i in 1:B){
  sams <- rnorm(n,mean(ss),sd(ss))
  mss[[i]] <- mean(sams)
}
#
mean(mss)
#
var(mss)
#
sd(mss)
#or 
test2 <- unlist(map(1:B,function(i){sams <- rnorm(n,mean(ss),sd(ss))
mss2 <- mean(sams)}))
```


### Distribution of the Parametric Bootstrapp Estimates

```{r}
hist(mss)
```


### Bootstrapping The Standard Error of The COrrelation Coefficient

```{r}
#dataset
practice <- tibble(x=c(29,435,86,1090,219,503,47,3524,185,98,952,89),
                   y=c(127,214,133,208,153,184,130,217,141,154,194,103))
#observed correlation
cor(practice$x,practice$y)
#scatter plot
plot(practice$x,practice$y)
B <- 5000
out <- vector("numeric",B)
nsamp8 <- vector("numeric",B)
for(i in 1:B){
  rs <- sample(1:length(practice$x),length(practice$x),replace = T)
  #number of times observation 8 was sampled
  nsamp8[[i]] <- sum(rs==8)
  #sim <- practice[rs,]
  sim <- practice %>% slice(rs)
  out[[i]] <- cor(sim$x,sim$y)
}
#correlation estimate
mean(out)
#variance of bootstrap replicate
var(out)
#sd of bootstrap replicate
sd(out)
```

### Distribution of the Bootstrapped Correlations

```{r}
hist(out)
```

### Influence of Observation 8

```{r}
#correlation without observation 8
practice2 <- practice %>% filter(x < 2000 & y < 2000)
cor(practice2$x,practice2$y)
#Number of times Observation 8 was sampled
plot(1:B,nsamp8,main='Number of Times Sample 8 was Sampled')
#Correlation vs Number of Times Sample 8 was taken
boxplot(out~factor(nsamp8),col=5)
```

### Estimation of Bias via Bootstrapping

Bias is defined as how far the estimator is from the true parameter on average, i.e. $$bias(\hat{\theta}) = E(\hat{\theta}) - \theta$$ and estimate is obtained by replacing $\theta$ by its plugin value which gives; $$\hat{bias(\hat{\theta})} = E(\hat{\theta}) - \hat{\theta}$$.

### Example of the Patch Data

```{r}
patchdata <- tibble(placebo = c(9243,9671,11792,13357,9055,6290,12412,18806),
                    old=c(17649,12013,19979,21816,13850,9806,17208,29044),
                    new=c(16449,14614,17274,23798,12560,10157,16570,26325))
patchdata <- patchdata %>% mutate(y=new-old,z=old-placebo)
observed_estimate <- mean(patchdata$y)/mean(patchdata$z)
```

####  Bootstrap Approach

```{r}
set.seed(500)
B <- 5000
tethas <- vector("numeric",B)
for(i in 1:B){
  drawn <- sample(1:length(patchdata$placebo),replace=T)
  dat <- patchdata %>% slice(drawn)
  tethas[i] <- mean(dat$y)/mean(dat$z)
}
bias <- mean(tethas) - observed_estimate
#distribution of the tethas
hist(tethas,nclass=30,probability = T)
abline(v=observed_estimate,col=2)
abline(v=mean(tethas),col=3,lty=2)
#confidence intervals
quantile(tethas,c(0.025,0.975))
```

### Bootstrap Confidence Interval

There are four ways to construct confidence intervals via bootstap which are;

1. Bootstrap t confidence interval (non parametric)
2. Bootstrap standard normal confidence interval (parametric)
3. Bootsrap percentile confidence interval (parametric or non-parametric)
4. Bca method

#### Bootstrap t Confidence Intervals

```{r}
mousedata <- tibble(z=c(94,197,16,38,99,141,23),
                    y=c(52,104,146,10,51,30,40))
#classical confidence intervals
tethahat <- mean(mousedata$z)
serror <- sd(mousedata$z)/sqrt(length(mousedata$z))
n <- length(mousedata$z)
tethahat + c(-1,1 )*qnorm(0.95)*serror
#bootstrap t
B <- 50000
zs <- numeric(B)
set.seed(5000)
for(i in 1:B){
  sims <- sample(mousedata$z,n,replace = T)
  zs[[i]] <- (mean(sims) - tethahat)/serror
}
#or
fry <- unlist(map(1:B, function(i){sims <- sample(mousedata$z,n,replace = T)
(mean(sims) - tethahat)/serror}))
#lower
tethahat - abs(quantile(zs,0.025))*serror
#higher
tethahat + quantile(zs,0.975)*serror
###
#lower
tethahat - abs(quantile(fry,0.025))*serror
#higher
tethahat + quantile(fry,0.975)*serror
```



















